{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Machine translation and related technologies.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "431E9AECF60849679FA0CDB3B39C25D5",
        "jupyter": {},
        "tags": [],
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "6f7ecf0f-e126-46ad-9050-5ec743de1d15"
      },
      "source": [
        "import os\n",
        "os.listdir('/home/kesci/input/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fraeng6506', 'd2l9528', 'd2l6239']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "F99F306F631B4E2F8F3464EEE0234CE1",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/home/kesci/input/d2l9528/')\n",
        "import collections\n",
        "import d2l\n",
        "import zipfile\n",
        "from d2l.data.base import Vocab\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "C4C3229D46004ECF93F03C6BBC48AEDC",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "0ccb36b9-7c7b-4a72-bf5c-c5d4fa10e9bc"
      },
      "source": [
        "with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n",
        "      raw_text = f.read()\n",
        "print(raw_text[0:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "Who?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\n",
            "Wow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\n",
            "Fire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\n",
            "Help!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\n",
            "Jump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\n",
            "Stop!\tÇa suffit !\tCC-BY 2.0 (France) Attribution: tato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "BB101036A28849798E95206AAE93FDFC",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "a2727eaa-987d-401f-a460-f7e538f75f05"
      },
      "source": [
        "def preprocess_raw(text):\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "    out = ''\n",
        "    for i, char in enumerate(text.lower()):\n",
        "        if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
        "            out += ' '\n",
        "        out += char\n",
        "    return out\n",
        "\n",
        "text = preprocess_raw(raw_text)\n",
        "print(text[0:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "go .\tva !\tcc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev)\n",
            "hi .\tsalut !\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji)\n",
            "hi .\tsalut .\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux)\n",
            "run !\tcours !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic)\n",
            "run !\tcourez !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic)\n",
            "who?\tqui ?\tcc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux)\n",
            "wow !\tça alors !\tcc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)\n",
            "fire !\tau feu !\tcc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic)\n",
            "help !\tà l'aide !\tcc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko)\n",
            "jump .\tsaute .\tcc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)\n",
            "stop !\tça suffit !\tcc-b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "AC2D82A7081943B0BE94018061BC62AF",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "e83cd5f1-0585-42f4-ef99-b0b4cdf6a20e"
      },
      "source": [
        "num_examples = 50000\n",
        "source, target = [], []\n",
        "for i, line in enumerate(text.split('\\n')):\n",
        "    if i > num_examples:\n",
        "        break\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) >= 2:\n",
        "        source.append(parts[0].split(' '))\n",
        "        target.append(parts[1].split(' '))\n",
        "        \n",
        "source[0:3], target[0:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['go', '.'], ['hi', '.'], ['hi', '.']],\n",
              " [['va', '!'], ['salut', '!'], ['salut', '.']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "7589E7D345B3463A8F0F4574ED6EDA9A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "fc870fe4-30ab-4524-820d-1cbb234bb448"
      },
      "source": [
        "d2l.set_figsize()\n",
        "d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=['source', 'target'])\n",
        "d2l.plt.legend(loc='upper right');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "text/html": [
              "<img src=\"https://cdn.kesci.com/rt_upload/7589E7D345B3463A8F0F4574ED6EDA9A/q5jefa8ffq.svg\">"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "D432CD6C4A644196831393AEF3EF6A06",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "dd64f9e8-3e43-47e7-944b-54d534641369"
      },
      "source": [
        "def build_vocab(tokens):\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)\n",
        "\n",
        "src_vocab = build_vocab(source)\n",
        "len(src_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "E556507EFE624C93B9A181F68B2CF64A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "8eee852e-cc97-40f7-c174-4f40463f0acf"
      },
      "source": [
        "def pad(line, max_len, padding_token):\n",
        "    if len(line) > max_len:\n",
        "        return line[:max_len]\n",
        "    return line + [padding_token] * (max_len - len(line))\n",
        "pad(src_vocab[source[0]], 10, src_vocab.pad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "E704BB99768F474E81871942B81E0665",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_array(lines, vocab, max_len, is_source):\n",
        "    lines = [vocab[line] for line in lines]\n",
        "    if not is_source:\n",
        "        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n",
        "    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n",
        "    valid_len = (array != vocab.pad).sum(1) #第一个维度\n",
        "    return array, valid_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "7BB02BF246AC4302824AA6CFA95A40D8",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_nmt(batch_size, max_len): # This function is saved in d2l.\n",
        "    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n",
        "    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n",
        "    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n",
        "    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "    return src_vocab, tgt_vocab, train_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "DF377DA83D8C409CBC7A48ABDC3CCFDB",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "ef03d0f8-468a-4beb-c4d8-bfdb7772af61"
      },
      "source": [
        "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)\n",
        "for X, X_valid_len, Y, Y_valid_len, in train_iter:\n",
        "    print('X =', X.type(torch.int32), '\\nValid lengths for X =', X_valid_len,\n",
        "        '\\nY =', Y.type(torch.int32), '\\nValid lengths for Y =', Y_valid_len)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = tensor([[   5,   24,    3,    4,    0,    0,    0,    0],\n",
            "        [  12, 1388,    7,    3,    4,    0,    0,    0]], dtype=torch.int32) \n",
            "Valid lengths for X = tensor([4, 5]) \n",
            "Y = tensor([[   1,   23,   46,    3,    3,    4,    2,    0],\n",
            "        [   1,   15,  137,   27, 4736,    4,    2,    0]], dtype=torch.int32) \n",
            "Valid lengths for Y = tensor([7, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "86647F7276B743F1B4D466AF8055C27A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "A894858F5FA94C3599B3CE9D4EF88964",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "26FFA2FB4ED04DC6BA3BAAA4A522217C",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(EncoderDecoder, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
        "        return self.decoder(dec_X, dec_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "2296785759574D6C86BD5DFBBC0A2E90",
        "mdEditEnable": false,
        "colab_type": "text"
      },
      "source": [
        "可以应用在对话系统、生成式任务中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "7886803BFCBC4D85A79A20B43FC5711A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqEncoder(d2l.Encoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
        "        self.num_hiddens=num_hiddens\n",
        "        self.num_layers=num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n",
        "   \n",
        "    def begin_state(self, batch_size, device):\n",
        "        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),\n",
        "                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]\n",
        "    def forward(self, X, *args):\n",
        "        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n",
        "        X = X.transpose(0, 1)  # RNN needs first axes to be time\n",
        "        # state = self.begin_state(X.shape[1], device=X.device)\n",
        "        out, state = self.rnn(X)\n",
        "        # The shape of out is (seq_len, batch_size, num_hiddens).\n",
        "        # state contains the hidden state and the memory cell\n",
        "        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n",
        "        return out, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "F48A7CA0591641A5A0AC12882EA3DA88",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "ad41e6d7-e9a8-4582-fec5-26756c5db2c1"
      },
      "source": [
        "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
        "X = torch.zeros((4, 7),dtype=torch.long)\n",
        "output, state = encoder(X)\n",
        "output.shape, len(state), state[0].shape, state[1].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "75A71227061547E0AF5EA9A5EDAE77F3",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqDecoder(d2l.Decoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0, **kwargs):\n",
        "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n",
        "        self.dense = nn.Linear(num_hiddens,vocab_size)\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        return enc_outputs[1]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        X = self.embedding(X).transpose(0, 1)\n",
        "        out, state = self.rnn(X, state)\n",
        "        # Make the batch to be the first dimension to simplify loss computation.\n",
        "        out = self.dense(out).transpose(0, 1)\n",
        "        return out, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "8B922C6FCF5643CFA46ECB509A109C8B",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "ae3215f3-0124-4824-fc86-3bcfc42f73b5"
      },
      "source": [
        "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
        "state = decoder.init_state(encoder(X))\n",
        "out, state = decoder(X, state)\n",
        "out.shape, len(state), state[0].shape, state[1].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "28493F1E5E1B47CA9FD60AF2AF1E2491",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SequenceMask(X, X_len,value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n",
        "    X[~mask]=value\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "EAAF6E804ADA42A8892B4445F503E004",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "704c57e1-05f3-4a7d-a0d2-048d992242bf"
      },
      "source": [
        "X = torch.tensor([[1,2,3], [4,5,6]])\n",
        "SequenceMask(X,torch.tensor([1,2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [4, 5, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "8AA5E43A0A8F4FD38C6E5AF16A747C6C",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "1846c263-99ae-4914-88b6-ea3ccf265b11"
      },
      "source": [
        "X = torch.ones((2,3, 4))\n",
        "SequenceMask(X, torch.tensor([1,2]),value=-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  1.,  1.,  1.],\n",
              "         [-1., -1., -1., -1.],\n",
              "         [-1., -1., -1., -1.]],\n",
              "\n",
              "        [[ 1.,  1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.,  1.],\n",
              "         [-1., -1., -1., -1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "2FA64864BA224A49B54C188323F7044B",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "    # pred shape: (batch_size, seq_len, vocab_size)\n",
        "    # label shape: (batch_size, seq_len)\n",
        "    # valid_length shape: (batch_size, )\n",
        "    def forward(self, pred, label, valid_length):\n",
        "        # the sample weights shape should be (batch_size, seq_len)\n",
        "        weights = torch.ones_like(label)\n",
        "        weights = SequenceMask(weights, valid_length).float()\n",
        "        self.reduction='none'\n",
        "        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n",
        "        return (output*weights).mean(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "F59552886EC2485E8567BCD3C33F6D06",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "5f0dffc4-3a3a-4e2c-f5e7-b5809cb42cd8"
      },
      "source": [
        "loss = MaskedSoftmaxCELoss()\n",
        "loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.3026, 1.7269, 0.0000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "158F3AF8015D44F6891C579EAE3AC2F6",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss = MaskedSoftmaxCELoss()\n",
        "    tic = time.time()\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        l_sum, num_tokens_sum = 0.0, 0.0\n",
        "        for batch in data_iter:\n",
        "            optimizer.zero_grad()\n",
        "            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]\n",
        "            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1\n",
        "            \n",
        "            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n",
        "            l = loss(Y_hat, Y_label, Y_vlen).sum()\n",
        "            l.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                d2l.grad_clipping_nn(model, 5, device)\n",
        "            num_tokens = Y_vlen.sum().item()\n",
        "            optimizer.step()\n",
        "            l_sum += l.sum().item()\n",
        "            num_tokens_sum += num_tokens\n",
        "        if epoch % 50 == 0:\n",
        "            print(\"epoch {0:4d},loss {1:.3f}, time {2:.1f} sec\".format( \n",
        "                  epoch, (l_sum/num_tokens_sum), time.time()-tic))\n",
        "            tic = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "08CBFD23FD6C40E890E96ADFA0FE161F",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "35eb90f8-a58d-4c13-dc5f-a4c694f8865d"
      },
      "source": [
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
        "batch_size, num_examples, max_len = 64, 1e3, 10\n",
        "lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\n",
        "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(\n",
        "    batch_size, max_len,num_examples)\n",
        "encoder = Seq2SeqEncoder(\n",
        "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder(\n",
        "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "model = d2l.EncoderDecoder(encoder, decoder)\n",
        "train_ch7(model, train_iter, lr, num_epochs, ctx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   50,loss 0.093, time 38.2 sec\n",
            "epoch  100,loss 0.046, time 37.9 sec\n",
            "epoch  150,loss 0.032, time 36.8 sec\n",
            "epoch  200,loss 0.027, time 37.5 sec\n",
            "epoch  250,loss 0.026, time 37.8 sec\n",
            "epoch  300,loss 0.025, time 37.3 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "13CC26D191FF43BF8BB79FB14CDF7BAF",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
        "    src_len = len(src_tokens)\n",
        "    if src_len < max_len:\n",
        "        src_tokens += [src_vocab.pad] * (max_len - src_len)\n",
        "    enc_X = torch.tensor(src_tokens, device=device)\n",
        "    enc_valid_length = torch.tensor([src_len], device=device)\n",
        "    # use expand_dim to add the batch_size dimension.\n",
        "    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)\n",
        "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n",
        "    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)\n",
        "    predict_tokens = []\n",
        "    for _ in range(max_len):\n",
        "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
        "        # The token with highest score is used as the next time step input.\n",
        "        dec_X = Y.argmax(dim=2)\n",
        "        py = dec_X.squeeze(dim=0).int().item()\n",
        "        if py == tgt_vocab.eos:\n",
        "            break\n",
        "        predict_tokens.append(py)\n",
        "    return ' '.join(tgt_vocab.to_tokens(predict_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {},
        "tags": [],
        "id": "D978A9B7B69B4E99942AE11491A7273B",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "01df1b98-1944-4290-f6c7-5f468e41d08a"
      },
      "source": [
        "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
        "    print(sentence + ' => ' + translate_ch7(\n",
        "        model, sentence, src_vocab, tgt_vocab, max_len, ctx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go . => va !\n",
            "Wow ! => <unk> !\n",
            "I'm OK . => ça va .\n",
            "I won ! => j'ai gagné !\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}