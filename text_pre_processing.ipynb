{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "text pre-processing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_ytfpat1",
        "jupyter": {},
        "tags": [],
        "id": "7FA4C53DED4F42279EA3AB3229B88DB7",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "034180d9-16e6-4a9b-c9a3-02766eece1b3"
      },
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "def read_time_machine():\n",
        "    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n",
        "        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n",
        "    return lines\n",
        "\n",
        "\n",
        "lines = read_time_machine()\n",
        "print('# sentences %d' % len(lines))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# sentences 3221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_z5grfxp",
        "jupyter": {},
        "tags": [],
        "id": "F80F8AFC1C0A48BDB66D52A18DC3A940",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "86c44d0f-c870-48be-a849-73b35bf636d2"
      },
      "source": [
        "def tokenize(sentences, token='word'):\n",
        "    \"\"\"Split sentences into word or char tokens\"\"\"\n",
        "    if token == 'word':\n",
        "        return [sentence.split(' ') for sentence in sentences]\n",
        "    elif token == 'char':\n",
        "        return [list(sentence) for sentence in sentences]\n",
        "    else:\n",
        "        print('ERROR: unkown token type '+token)\n",
        "\n",
        "tokens = tokenize(lines)\n",
        "tokens[0:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "graffitiCellId": "id_wapwqkb",
        "jupyter": {},
        "tags": [],
        "id": "37532FBF89C242A1805534BBE05C343A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
        "        counter = count_corpus(tokens)  # : \n",
        "        self.token_freqs = list(counter.items())\n",
        "        self.idx_to_token = []\n",
        "        if use_special_tokens:\n",
        "            # padding, begin of sentence, end of sentence, unknown\n",
        "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
        "            self.idx_to_token += ['', '', '', '']\n",
        "        else:\n",
        "            self.unk = 0\n",
        "            self.idx_to_token += ['']\n",
        "        self.idx_to_token += [token for token, freq in self.token_freqs\n",
        "                        if freq >= min_freq and token not in self.idx_to_token]\n",
        "        self.token_to_idx = dict()\n",
        "        for idx, token in enumerate(self.idx_to_token):\n",
        "            self.token_to_idx[token] = idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "def count_corpus(sentences):\n",
        "    tokens = [tk for st in sentences for tk in st]\n",
        "    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "23"
        },
        "graffitiCellId": "id_hm9y6bm",
        "jupyter": {},
        "tags": [],
        "id": "1BE94FF518DB4C4A8CDFB95C0262B47E",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "4f33ba37-5b4a-4658-bc98-5de306d13ea9"
      },
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_k48bsl2",
        "jupyter": {},
        "tags": [],
        "id": "9FBB71C21B5C4F5283C70CFE3BB07112",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "d770d2dd-2136-484d-e84d-4b009da1d798"
      },
      "source": [
        "for i in range(8, 10):\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\n",
            "indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\n",
            "words: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
            "indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_7u3knll",
        "jupyter": {},
        "tags": [],
        "id": "46F5F57611E248ECB51F04BD0104E278",
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Mr. Chen doesn't agree with my suggestion.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_uz6civu",
        "jupyter": {},
        "tags": [],
        "id": "30D69C6B1BE44362BA556E2E5EEF493A",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "bf1709b7-8b47-4566-eb58-6787b9d17bd1"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "graffitiCellId": "id_r13iwga",
        "jupyter": {},
        "tags": [],
        "id": "B83D30D3670B44A38527B4943BE4DBE0",
        "scrolled": false,
        "colab_type": "code",
        "colab": {},
        "outputId": "dd443f67-9f27-45d0-9f20-ffc5e6bac6cf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import data\n",
        "data.path.append('/home/kesci/input/nltk_data3784/nltk_data')\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}